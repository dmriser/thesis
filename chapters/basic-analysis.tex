\chapter{Basic Analysis \& Corrections}

\section{Introduction}
This chapter discusses analysis procedures that are common to the subsequent data analyses of pions and kaons.  These procedures can be divided into two groups.  The first type of basic analysis described is the aggregation or calculation of scalar values over the run-period (examples include luminosity and helicity).  The second type of analysis procedure described is a correction to measured values.  Vertex corrections, timing corrections, and kinematic corrections will be discussed.


\section{Luminosity Calculation}
A useful concept in accelerator/collider physics is the luminosity $\mathcal{L}$.  Luminosity is defined as the number of collisions per unit area per unit time that could lead to some process of interest.  Consider as an example elastic scattering of electrons from protons.  The luminosity is the number of electron-proton collisions per unit time per unit area.  The rate $\frac{dN}{dt}$ of the occurance of events for some process $X$ can be written in terms of this luminisity and the cross section for the process.

\begin{equation}
	\frac{dN_X}{dt} = \mathcal{L} \sigma_X 
\end{equation}

For the fixed target case, the luminosity has a simple expression.

\begin{equation}
	\mathcal{L} = j_e \rho_p l_T 
\end{equation}

To find the total number of events which accumulate in some time $t_{exp}$ the event rate is integrated with respect to time.

\begin{equation}
	N_X = \integral_{0}^{t_{exp}} j_e \rho_p l_T \sigma_X dt = \rho_p l_T \sigma_X \integral_{0}^{t_{exp}} j_e dt =  \rho_p l_T \sigma_X \Delta Q
\end{equation}

Experimentally, the factor $\Delta Q$ can be calculated from charge deposition measurements performed by the Faraday Cup.  The Faraday Cup charge is a scalar value written periodically into the output event stream, not with every recorded event.  This information is stored in the output BOS files in a bank called \texttt{TRGS}, the variable is named \texttt{FCUP\_G2}.  \\

For this data analysis, the final ntuple (root files) used did not contain the Faraday Cup charge information.  For this reason, the authors used the BOS files directly and recorded the value of \texttt{FCUP\_G2} for every scalar reading, as well as the event number directly after each scalar entry (from the \texttt{HEAD} bank).  This event number correlates directly to the event number stored in the root files used for analysis. \\

The total accumulated charge over a run is then simply the sum over consecutive differences in the Faraday Cup charge.

\begin{equation}
	\Delta Q = \sum_{i=1}^{n-1} q[i+1]-q[i] 	
\end{equation}

Here n denotes the number of scalar entries for a given file.  Due to the periodic nature of the scalar bank writing events are also recorded after the last reading of the file, and before the first scalar reading of the next file in the run.  To account for this the difference between consecutive files last and first readings is added to the total. \\

For the E1-F dataset, a run typically contains around 20 files, each representing a raw file size of 2 Gigabytes.  These files are named by run number, and given an index from $0$ to $n_{files}-1$.  It is not uncommon that a run will contain missing files in the middle of the range.  If this occurs, the charge difference between last/first reading is not added to the total. \\

As an additional quality control measure, any charge which accumulates in a period of time where the number of events did not change is not added to the total.  Similarly, any events which occur within regions where no charge is recorded need to be discarded.  This is accomplished by writing out the bad event windows for every file and removing these events from our analysis. \\

The result of this procedure is a numerical value of charge for each run.  For those interested, it is important to note that this value needs to be scaled by the DAQ scaling factor before it represents a value of charge.  In our analysis, the numerical value of charge for a typical file is a few tens of $\muC$.

\section{Determination of Good Run List}
The total dataset contains 831 runs.  Due to the complex nature of the experimental setup, it is not uncommon for run conditions to change during a few of the runs such that the data collected are not of analysis quality.  Imagine as a simple example that the liquid hydrogen target boils, the density is suddenly decreased, and the number of recorded events drops drastically (but the Faraday Cup charge would look the same).  For this reason, a good run list is constructed and used in the analysis. \\

To construct this list, we simply count good electrons in every file and normalize that by the accumulated charge for that file.  While the number of events collected various from run to run this is a stable quantity -- provided that the run conditions do not vary greatly.  Runs which are within 3 standard deviations of the mean (calculated over the dataset) are used as good runs.  The good run list used for this analysis contains 522 runs.  

\section{Helicity Determination}
During the course of the E1-F run period the beam helicity convention was changed by the insertion of a half-wave plate at the injector.  Accordingly, our definition of $+$ and $-$ helicity must change in accordance with these wave-plate insertions.  To monitor these changes, the value of $A_{LU}^{\sin\phi}$ for $\pi^+$ is recorded for every run.  Whenever the asymmetry (which has a magnitude of around $3\%$) changes sign, we know that the sign convention has changed.  These changes are then taken into account in the data analysis.   

\section{Vertex Corrections}
Vertex information $(v_x, v_y, v_z)$ is calculated based on the intersection of a track with the midplane.  

\section{Timing Corrections}
Timing information for tracks comes from the time-of-flight detector system.  After normal calibration, small offsets in timing between time of flights paddles still exist for the E1-F dataset.  These biases can be removed on a run-by-run and paddle-by-paddle basis by adding a small shift $t_{corr}$.  In order to determine this shift $t_{corr}$ for each paddle, charged pions are used.  \\

Using momentum information from the drift chambers the time of flight can be predicted and the difference $\Delta t$ can be determined for each electron/pion pair. 

\begin{equation}
	\Delta t = t_{measured} - t_{predicted} = t_{measured} - \frac{d}{c} \sqrt{1-(m/p)^2} 
\end{equation} 

Here m is assumed to be $m_{\pi}$.  The offset $\Delta t$ from 0 defines the value of $t_{corr}$ for each paddle.  If this value is exceedingly small, no correction is applied.  For some paddles with low statistics a reasonable value for $t_{corr}$ cannot be obtained and these paddles are excluded from the analysis.  \\

In the method described above, the calibrated paddle is the one which is struck by the pion.  The electron paddle which was struck could also require calibration.  In practice the magnitude of the correction term $t_{corr}$ is small, and the paddle offset is (likely) randomly distributed about 0 from paddle to paddle.  By including events from many different (electron) paddles, miscalibration effects from the electron side cease to be important.  This is demonstrated by the success of the technique in centering the $\Delta t$ distributions.  \\

The work described in this section was performed by Nathan Harrison for the E1-F dataset.

\section{Kinematic Corrections}

